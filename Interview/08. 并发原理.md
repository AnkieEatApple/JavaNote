## 并发原理.md

##### 1. 目录
- [一、synchronized](#synchronized)
- [二、synchronized底层实现原理](#theory)
- [三、synchronized和ReentrantLock](#ReentrantLock)
- [四、jmm的内存可见性](#jmm)
- [五、CAS](#cas)
- [六、Java线程池](#threadpool)


### 一、<span id="synchronized">synchronized</span>
1. 线程安全问题的主要诱因
	* 存在共享数据(也称临界资源)
	* 存在多条线程共同操作这些共享数据
2. 解决问题的根本方法
	* 同一时刻有且只有一个线程在操作共享数据，其他线程必须等到该线程处理完数据后，再对共享数据进行操作
3. 互斥锁的特性
	* **互斥性**：即在同一时间只允许一个线程持有某个对象锁，通过这种特性来实现多线程的协调机制，这样在同一时间只有一个线程对需要同步的代码块(复合操作)进行访问。互斥性也称为操作的原子性。
	* **可见性**：必须确保在锁被释放之前，对共享变量所做的修改，对于随后获得该锁的另一个线程是可见的(即在获得锁时获得最新共享变量的值)，都则另一个线程可能是在本地缓存的某个副本上的继续操作，从而引起不一致
	* synchronized锁的不是代码，锁的是对象

#### 1.1 根据获取锁的分类：获取对象锁和获取类锁
1. 获取对象锁的两种用法
	1. 同步代码块(synchronized(this), synchronized(类实例对象))，锁是小括号()中的实例对象。
	2. 同步非静态代码块(synchronized method)，锁是当前对象的实例对象
	3. 实例代码逻辑为SyncTherad.java
2. 获取类锁的两种方法
	1. 同步代码块(synchronized类(类.class))，锁是小括号()中的类对象(Class对象)
	2. 同步静态方法(synchronized static method)，锁是当前的类对象(Class对象)
3. 对象锁和类锁的总结
	1. 有线程访问对象的同步代码块时，另外的线程可以访问该对象的非同步代码块；
	2. 若锁住的是同一个对象，一个线程在访问对象的同步代码块时，另一个访问对象的同步代码块的线程会被阻塞；
	3. 若锁住的是同一个对象，一个线程在访问对象的同步方法时，另一个访问对象的同步方法的线程会被阻塞；
	4. 若锁住的是同一个对象，一个线程在访问对象的同步代码块时，另一个访问对象的同步方法的线程被会被阻塞，反之亦然；
	5. 同一个类的不同对象的对象锁互不干扰；
	6. 类锁由于也是一种特殊的对象锁，因此表现和上述1，2，3，4一致，而由于一个类只有一把对象锁，所以同一个类的不同对象使用类锁将会是同步的；
	7. 类锁和对象锁互补干扰。


### 二、<span id="theory">synchronized底层实现原理</span>
1. 实现Synchronized的基础
	* Java对象头
	* Monitor
2. 对象在内存中的布局
	* 对象头
	* 实例数据
	* 对齐填充
3. 对象头的结构<br/>![WeChat429f7f81158a85c7e3f8835f38767979.png](https://i.loli.net/2019/08/21/giYX5HBE4ZOFaLJ.png)
4. MarkWord<br/>![WeChat9ee7f83f771cee8d4f78365594b66b59.png](https://i.loli.net/2019/08/21/D6tg4IFiRawhkr9.png)
5. Monitor：每个Java对象天生自带了一把看不见的锁 
6. Monitor锁的竞争、获取、与释放<br/>![WeChatf672f27c70ba643e34ea56e0e09af1c6.png](https://i.loli.net/2019/08/21/7HuOYSIU9GFNt4W.png) 
	* 编译后的class文件可以看到monitorenter进入，然后时monitorexit退出
	* 若是一个方法，查看该方法的flags时否存在`ACC_SYNCHRONIZED`
7. 什么是重入
	* 从互斥锁的设计上来说，放一个线程试图操作一个由其他的线程持有的对象锁的临界资源时，将会处于阻塞状态，但当以一个线程再次请求自己持有对象锁的临界资源时，这种情况属于重入。
8. 为什么会对synchronized嗤之以鼻
	* 早起版本中，synchronized属于重量级锁，依赖于native的Mutex Lock实现的
	* 线程之间的切换需要从用户态转换到核心态，开销较大
9. 什么是CAS
	* 比较和交换（Conmpare And Swap）是用于实现多线程同步的原子指令。
	* 将内存位置的内容与给定值进行比较，只有在相同的情况下，将该内存位置的内容修改为新的给定值。 这是作为单个原子操作完成的。 原子性保证新值基于最新信息计算; 
	* 如果该值在同一时间被另一个线程更新，则写入将失败。 操作结果必须说明是否进行替换; 这可以通过一个简单的布尔响应（这个变体通常称为比较和设置），或通过返回从内存位置读取的值来完成。
	* java1.5引进的。

#### 2.1 锁的分类
1. Java6以后，synchronized性能得到了很大的提升
	* Adaptive Spinning
	* Lock Eliminate
	* Lock Coarsening
	* Lightweight Locking
	* Biased Locking...
2. 自旋锁
	* 许多情况下，共享数据的锁定状态持续时间较短，切换线程不值得
	* 通过让线程执行忙循环等待锁的释放，不让出CPU
	* 缺点：若锁被其他线程长时间占用，会带来许多性能上的开销
	* 可以通过PreBlockSpin修改
3. 自适应自旋锁
	* 自旋的次数不再固定
	* 由前一次在同一个锁上的自旋时间及锁的拥有者的状态来决定

4. 锁消除
	* JIT编译时，对运行上下文进行扫描，取出不可能存在的竞争的锁。
	* 比如Stringbuffer中的append就是被synchronized修饰的，但是修饰的这个Stringbuffer并没有被其他线程共享的时候，这个锁就可以消除

5. 另一种极端
	* 情况环境：如果对同一个对象同步锁时，同时加锁很多很多次的话，JVM就会将锁粗话到这个加锁的外部

6. synchronized的四种状态
	* 无锁、偏向锁、轻量级锁、重量级锁
	* 锁膨胀方向：无锁->偏向锁->轻量级锁->重量级锁

7. 偏向锁：减少统一线程获取锁的代价 
	* 大多数情况下，锁不存在多线程竞争，总是由同一线程多次获得，偏向锁就对此进行了优化
	* **核心思想**：如果一个线程获得了锁，那么锁就进入偏向模式，此时Mark Word的结构也变为偏向锁结构，当该线程再次请求锁时，无需再做任何同步操作，即获取锁的过程只需检查Mark Word的锁标记位位偏向锁以及当前线程Id等于Mark Word的ThreadID即可，这样就省去了大量有关锁申请的操作。
	* 不适用于锁竞争比较激烈的多线程场合

8. 轻量级锁
	* 轻量级锁时由偏向锁升级来的，偏向锁运行在一个线程进入同步块的情况下，当第二个线程加入锁争用的时候，偏向锁就会升级为轻量级锁。
	* 适用场景：线程交替执行同步块
	* 若存在同一时间访问统一锁的情况，就会导致轻量级锁膨胀为重量级锁  

9. 锁的内存语义
	* 当线程释放锁时，Java内存模型会把该线程对应的本地内存中的共享变量刷新到主内存中
	* 而当线程获取锁时，Java内存模型会把该线程对应的本地内存置为无效，从而使得被监视器保护的临界区代码必须从主内存中读取共享变量。<br/>![WeChat90993cd471f3cec8534fc621ec619b34.png](https://i.loli.net/2019/08/22/TktD1pogMPO6evR.png)

#### 2.2 偏向锁、轻量级锁、重量级锁的汇总

|锁|优点|缺点|使用场景|
|:--:|:--:|:--:|:--:|
|偏向锁|加锁和解锁不需要CAS操作，没有额外的性能消耗，和执行非同步方法相比仅存在纳秒级的差距|如果线程间存在竞争，会带来额外的锁的撤销的消耗|只有一个线程访问同步块或者同步方法的场景|
|轻量级锁|若线程长时间抢不到锁，自旋会消耗CPU性能|线程交替执行同步块或者同步方法的场景|线程交替执行同步块或者同步方法的场景|
|重量级锁|线程竞争不使用自旋，不会消耗CPU|线程阻塞，响应时间缓慢，在多线程下，频繁的获取释放锁，会带来巨大的性能消耗|追求吞吐量，同步块或者同步方法执行时间较长的场景|



### 三、<span id="ReentrantLock">synchronized和ReentrantLock</span>
1. ReentrantLock(再入锁)
	* 位于java.util.concurrent.lock包
	* 和CountDownLatch、FutureTask、Semaphore一样基于AQS实现(AQS队列同步器)
	* 能够实现比synchronized更细粒度的控制，例如控制fairness(公平性)
	* 调用lock()之后，必须调用unlock()释放锁
	* 性能未必比synchronized高，并且也是可重入的

2. ReentrantLock公平性的设置
	* ReentrantLock fairLock = new ReentrantLock(true);
	* 参数为true时，倾向于将锁赋予等待时间最久的线程
	* 公平锁：获取锁的顺序按先后调用lock方法饿顺序(即排队打饭，慎用)
	* 非公平锁：抢占的顺序不一定，看运气
	* ReentrantLock在使用的时候最好使用try...cath...finally这个逻辑，在finally中解锁。

3. ReentrantLock将锁对象化
	* 判断是否有线程，或者某个特定线程，在排队等待获取锁
	* 带超时的获取锁的尝试
	* 感知有没有成功的获取锁

4. 是否能将wait\notify\notifyAll对象化
	* java.util.concurrent.locks.Condition
	* 其中ArrayBlockQueue是数组的有界阻塞队列，互斥锁是由ReentrantLock实现的

5. 总结
	* synchronized是关键字，ReentrantLock是类
	* ReentrantLock可以对获取锁的等待时间进行设置，避免死锁
	* ReentrantLock可以获取各种锁的信息
	* ReentrantLock可以灵活地实现多路通知
	* **机制**：sync操作Mark Word，lock调用Unsafe类的park()方法


### 四、<span id="jmm">jmm的内存可见性</span>
#### 4.1 什么是Java内存模型中的happens-before
1. Java内存模型JMM
	* Java内存模型(即Java Memory Model，简称JMM)本身是一种抽象的概念，并不真实存在，它的描述是一组规则或规范，通过这组规范定义了程序中各个变量(包括实例字段、静态字段和构成数组对象的元素)的访问方式。<br/>![WeChat3cb2a92b074c72360df93a3ce476caef.png](https://i.loli.net/2019/08/22/cvhk7ZKxNPM35Am.png)

#### 4.2 JMM中的主内存和工作内存
1. JMM中的主内存
	* 储存Java实例对象
	* 包括成员变量、类信息、常量、静态变量等
	* 属于数据贡献给的区域，多线程并发操作时，会引发线程安全问题

2. JMM中的工作内存
	* 储存当前方法的所有本地变量信息，本地变量对其他线程不可见
	* 字节码行号指示器，Native方法信息
	* 属于线程私有数据区域，不存在线程安全问题

3. JMM与Java内存区域划分时不同的概念层次
	* JMM描述的是一组规则，围绕原子性、有序性、可见性展开
	* 相似点：存在共享区域和私有区域

4. 主内存和工作内存的数据储存类型以及操作方式归纳
	* 方法里的基本数据类型本地变量将直接存储在工作内存的栈帧结构中
	* 引用类型的本地变量：引用储存在工作内存中，实例储存在内存中
	* 成员变量、static变量、类信息均会被存储在主内存中
	* 主内存共享的方式是线程各拷贝一份数据到工作内存，操作完成后刷新回到主内存

#### 4.3 JMM如何解决可见性问题
1. 需要解决的问题：
	* 当线程共享变量的时候，情况比较复杂，如果处理器对某个变量进行了修改，可能只是体现可该内核的缓存里，这是个本地状态，而运行在其他内核上的线程，可能加载的是旧状态，这就会导致一致性的问题。<br/>
	* 从理论上说，多线程的共享引入了复杂的数据依赖性，不管编译期处理器怎么做，重排序，都必须尊重数据依赖性的要求，否则就打破了数据的依赖性![WeChate16f91c70263768826913f2f058660c5.png](https://i.loli.net/2019/08/22/LYbVfQlICroeDTU.png)
2. 指令重排序需要满足的条件
	* 在单线程环境下不能改变程序运行的结果
	* 若存在数据依赖关系的不允许重排序，即无法通过happens-before原则推导出来的，才能进行指令的重排序

3. A操作的结果要对B操作可见，则A与B存在happens-before关系
	* i = 1;	// 线程A执行
	* j = 1;	// 线程B执行

4. happens-before的八大原则
	1. 程序次序原则：一个线程内，按照代码顺序，书写在前面的操作先行发生于书写在后面的操作；
	2. 锁定规则：一个unLock操作先行发生于后面对同一个锁的Lock操作；
	3. volatile变量规则：对于一个变量的写操作先行发生于后面对这个变量的读操作；
	4. 传递规则：如果操作A先行发生于操作B，而操作B又先行发生于操作C，责可以得出操作A先行发生于操作C；
	5. 线程启动规则：Thread对象的start()方法先行发生于此线程的每一个动作；
	6. 线程中断规则：对线程interrupt()方法的调用先行发生于被中断线程的代码检测到中断事件的发生；
	7. 线程终结规则：线程中所有的操作都先行发生于线程的终止检测，我们可以通过Thread.join()方法结束、Thread.isAlive()的返回值手段检测到线程已经终止执行；
	8. 对象终结规则：一个对象的初始化完成先行发生于他的finalize()方法的开始；

5. happens-before的概念
	* 如果两个操作不满足上述任意一个happens-before规则，那么这两个操作就没有顺序的保障，JVM可以对这两个操作进行重排序；
	* 如果操作A happens-before操作B，那么操作A在内存上所做的操作对操作B都是可见的

6. volatile：JVM提供的轻量级同步机制
	* 保证被volatile修饰的共享变量对所有线程总是可见的
	* 禁止指令的重排序优化

7. volatile的可见性
	* i++并不具备原子性，即使被volatile修饰，因为javap之后，i++是分两步，一步是读取i的值，++操作后，再写回道栈中。
	* 解决办法就是在操作i的方法前面添加synchronized关键字

	```
	// value++并没有原子性，多线程操作就乱了
	public class VolatileVisbility {
		public static volatile int value = 0;
		
		public static void increase() {
			value++;
		}
	}
	// 修改办法，在increase方法中添加synchronized
	public class VolatileVisbility {
		public static int value = 0;
		
		public synchronized static void increase() {
			value++;
		}
	}
	
	// 原子性操作的可以使用volatile修饰的
	public class VolatileSafe {
		volatile boolean shutdown;
		public void close() {
			shutdown = true;
		}
		public void doWork() {
			while (!shutdown) {
				System.out.println("safe...");
			}
		}
	}
	```

8. volatile变量为何立即可见？
	* 当写一个voliatle变量时，JMM会把该线程对应的工作内存中的共享变量值刷新到主内存中；
	* 当读取一个volatile变量时，JMM会把该线程对应的工作内存置为无效

9. volatile如何禁止重排优化，内存屏障(Memory Barrier)
	* 保证特定的操作的执行顺序
	* 保证某些变量的内存可见性
	* 通过插入内存屏障指令禁止在内存屏障的前后的指令执行重排优化
	* 强制刷出各种CPU的缓存数据，因此任何CPU上的线程都能读取到这些数据的最新版本
	* 最经典的就是单例的双重检测实现

#### 4.4 volatile和synchronized的区别
1. volatile本质是高数JVM当前变量在寄存器(工作内存)中的值是不确定的，需要从主存中读取；synchronized则是锁定当前变量，只有当前线程可以访问该变量，其他线程被阻塞住知道该线程完成变量操作为止
2. volatile仅能使用在变量级别；synchronized则可以使用在变量、方法和类级别
3. volatile仅能实现变量的修改可见性，不能保证原子性；而synchronized则可以保证变量修改的可见性和原子性
4. volatile不会操成线程的阻塞，synchronized可能会造成线程的阻塞
5. volatile标记的变量不会被编辑器优化，synchronized标记的变量可以被编辑器优化

### 五、<span id="cas">CAS(Compare and Swap)</span>
1. synchronized属于**悲观锁**，会默认线程会抢占共享变量，发生并发冲突，会屏蔽一切影响数据完成性的操作
2. 乐观锁，默认是不会引起并发冲突，因此只在提交操作时是否违反数据完整性，如果提交失败，则会记进行重试，比如CAS

3. CAS，一种高效实现线程安全的方法
	* 支持原子更新操作，适用于计数器，序列发生器等场景；
	* 属于乐观锁机制，号称lock-free
	* CAS操作失败时由开发者决定是否继续尝试，还是执行别的操作

4. CAS思想
	* 包含三个操作数：内存位置(V)、预期原值(A)、新值(B)

5. CAS多数情况下对开发者来说是透明的
	* J.U.C的atomic包提供了常用的原子性数据类型以及引用、数组的那个相关原子类型和更新操作工具，是多线程安全程序的首选。
	* Unsafe类虽提供CAS服务，但因能够操作任意内存地址读写而有隐患
	* Java9之后，可以使用Variable Handle API来替代Unsafe

6. 缺点
	* 若循环事件长，则开销很大
	* 只能保证一个共享变量的原子操作
	* ABA问题，就是A在是不是曾经改变为B再改变为A的，还是本来就是A，CAS就会默认A重来没有被改变过，这个漏洞解决办法，AtiomicStampedReference可以解决，可以查一下


### 六、<span id="threadpool">Java线程池</span>
1. 为什么要使用线程池
	1. 降低资源消耗
	2. 提高线程的可管理性

#### 6.1 利用Executots创建不同的线程池满足不同的场景的需求
1. **newFixedThreadPool(int nThreads)**，指定工作线程的数量的线程池，任务超出放到队列中
2. **newCacheThreadPool()**，处理大量短时间工作任务的线程池
	1. 试图缓存线程并重用，当无缓存线程可用时，就会创建新的工作线程；
	2. 如果线程闲置的时间超过阈值(一般60s)，则会被终止并移除缓存；
	3. 系统长时间闲置的时候，不会消耗什么资源
3. newSingleThreadExecutor()，创建唯一的工作线程来执行任务，如果线程异常结束，会有另一个线程取代它，保证任务顺利执行
4. newSingleThreadScheduledExecutor()与newScheduledThreadPool(int corePoolSize)，定时或者周期性的工作制度，两者的区别在于单一工作线程还是多个线程
5. newWorkStealingPool()，JDK8才引入的，内部会构建ForkJoinPool，利用working-stealing算法，并行的处理任务，不保证处理顺序


#### 6.2 Fork/Join框架
1. 将大任务分割若干个小任务执行并行，最终汇总每个小任务结果后得到大任务结果的框架。
2. 是Executor的一种具体实现，Work-Stealing算法，即某个线程从其他队列里窃取任务来执行<br/>![WeChat094c6c02c67333c1d852b53d41ad17c8.png](https://i.loli.net/2019/08/22/IyNRxD1X3tJkqVz.png)



#### 6.3 J.U.C的三个Executor接口

1. Executor：运行新任务的简单接口，将任务提交和任务执行细节解耦
2. ExecutorService：具备**管理执行器**和**任务生命周期的方法**，提交任务机制更完善
3. ScheduledExecutorService：支持Future和定期执行任务
3. 实现的逻辑图<br/>![WeChate68a9062093339b7d330dea42a3a3689.png](https://i.loli.net/2019/08/22/BRJ6TzQVD3jCurE.png)

#### 6.4 ThreadPoolExecutor实现原理
1. 实现原理逻辑图<br/>![WeChat7372312d01d34f7111aa4f5027da609a.png](https://i.loli.net/2019/08/22/3G7WLmQVa9sE1fg.png)
2. ThreadPoolExecutor的构造函数
	* coreSize：核心线程的数量
	* maximumPoolSize：线程不够用时能够创建的最大线程数
	* workQueue：任务等待队列
	* keepAliveTime：抢占的顺序不一定，看运气
	* threadFactory：创建新线程，Executors.defaultThreadFactory()
3. handler：线程池的饱和策略
	* AbortPolicy：直接抛出异常，这是默认策略
	* CallerRunsPolicy：用调用者所在的线程来执行任务
	* DiscardOldestPolicy：丢弃队列中最靠前的任务，并执行当前任务
	* DiscardPolicy：直接丢弃任务
	* 实现RejectedExecutionHandler接口的自定义handler
4. 新任务提交execute执行后的判断
	* 如果运行的线程少于corePoolSize，则创建新线程来处理任务，即使线程池中的其他线程是空闲的
	* 如果线程中的线程数量大于等于corePoolSize且小于maximumPoolSize，则只有当workQueue满时才创建新的线程去处理任务
	* 如果设置corePoolSize和maximumPoolSize相同，则创建的线程池的大小时固定的，这时如果有新任务提交，若workQueue未满，则将请求放入workQueue中，等待有空闲的线程去从workQueue中取任务并处理；
	* 如果运行的线程数量大于等于maximumPoolSize，这时如果workQueue已经满了，则通过handler所执行的策略来处理任务；
	* 流程图<br/>![WeChat9254ec670daba7bc9ee02c8ae1cc3023.png](https://i.loli.net/2019/08/22/6QYnqpKD5gJ1Wko.png)

#### 6.5 线程池
1. 线程池的状态
	* RUNNING：能接受新提交的任务，并且也能处理阻塞队列中的任务
	* SHUTDOWN：不再接受新提交的任务，但可以处理存量任务
	* STOP：不再接受新提交的任务，也不处理存量任务
	* TIDYING：所有的任务都已终止
	* TERMINATED：terminated()方法执行完后进入该状态
	* 状态转换图<br/>![WeChatdc4157bf33efffcc0a7d3d333c3a8335.png](https://i.loli.net/2019/08/22/kFJVWeuRhB6aEqj.png)
2. 工作线程的生命周期<br/>![WeChat275f6a9a9338666b6535631fd95110fb.png](https://i.loli.net/2019/08/22/vGCdAiSn7DZzOju.png)
3. 线程池的大小如何选定
	* CPU密集型：线程数=按核数或者核数+1设定
	* I/O密集型：线程数=CPU核数*(1 + 平均等待时间/平均工作时间)











